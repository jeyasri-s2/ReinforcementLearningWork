{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of practice_reinforce.ipynb","provenance":[{"file_id":"https://github.com/yandexdataschool/Practical_RL/blob/coursera/week5_policy_based/practice_reinforce.ipynb","timestamp":1596470713602}]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"DXgmnvQuCCpQ"},"source":["# REINFORCE in TensorFlow\n","\n","Just like we did before for Q-learning, this time we'll design a TensorFlow network to learn `CartPole-v0` via policy gradient (REINFORCE).\n","\n","Most of the code in this notebook is taken from approximate Q-learning, so you'll find it more or less familiar and even simpler."]},{"cell_type":"code","metadata":{"id":"bTXJDBeECCpR","executionInfo":{"status":"ok","timestamp":1596470129387,"user_tz":420,"elapsed":16522,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"3296cc14-0731-437f-ce23-6a79ae599a69","colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["import sys, os\n","if 'google.colab' in sys.modules:\n","    %tensorflow_version 1.x\n","    \n","    if not os.path.exists('.setup_complete'):\n","        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/spring20/setup_colab.sh -O- | bash\n","\n","        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/grading.py -O ../grading.py\n","        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week5_policy_based/submit.py\n","\n","        !touch .setup_complete\n","\n","# This code creates a virtual display to draw game images on.\n","# It will have no effect if your machine has a monitor.\n","if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n","    !bash ../xvfb start\n","    os.environ['DISPLAY'] = ':1'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n","Selecting previously unselected package xvfb.\n","(Reading database ... 144487 files and directories currently installed.)\n","Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.4_amd64.deb ...\n","Unpacking xvfb (2:1.19.6-1ubuntu4.4) ...\n","Setting up xvfb (2:1.19.6-1ubuntu4.4) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Starting virtual X frame buffer: Xvfb.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EKVMDPXiCCpU"},"source":["import gym\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"npouvHZpCCpY"},"source":["A caveat: we have received reports that the following cell may crash with `NameError: name 'base' is not defined`. The [suggested workaround](https://www.coursera.org/learn/practical-rl/discussions/all/threads/N2Pw652iEemRYQ6W2GuqHg/replies/te3HpQwOQ62tx6UMDoOt2Q/comments/o08gTqelT9KPIE6npX_S3A) is to install `gym==0.14.0` and `pyglet==1.3.2`."]},{"cell_type":"code","metadata":{"id":"FQdAhbQYCCpY","executionInfo":{"status":"ok","timestamp":1596470131195,"user_tz":420,"elapsed":16231,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"ba18c7f6-a73d-468c-f411-efbddce9b31c","colab":{"base_uri":"https://localhost:8080/","height":286}},"source":["env = gym.make(\"CartPole-v0\")\n","\n","# gym compatibility: unwrap TimeLimit\n","if hasattr(env, '_max_episode_steps'):\n","    env = env.env\n","\n","env.reset()\n","n_actions = env.action_space.n\n","state_dim = env.observation_space.shape\n","\n","plt.imshow(env.render(\"rgb_array\"))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7fa7b5c14550>"]},"metadata":{"tags":[]},"execution_count":3},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"i5wPJpSOCCpb"},"source":["# Building the network for REINFORCE"]},{"cell_type":"markdown","metadata":{"id":"4eIyPbceCCpb"},"source":["For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n","\n","For numerical stability, please __do not include the softmax layer into your network architecture__.\n","We'll use softmax or log-softmax where appropriate."]},{"cell_type":"code","metadata":{"id":"hhYAS4IhCCpc"},"source":["import tensorflow as tf\n","\n","sess = tf.InteractiveSession()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"axVr9Nb_CCpf"},"source":["# create input variables. We only need <s, a, r> for REINFORCE\n","ph_states = tf.placeholder('float32', (None,) + state_dim, name=\"states\")\n","ph_actions = tf.placeholder('int32', name=\"action_ids\")\n","ph_cumulative_rewards = tf.placeholder('float32', name=\"cumulative_returns\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X1VKfkSwCCph"},"source":["from keras.models import Sequential\n","from keras.layers import Dense\n","\n","network = Sequential()\n","network.add(Dense(32, activation='relu', input_shape=state_dim))\n","network.add(Dense(32, activation='relu'))\n","network.add(Dense(n_actions, activation='linear'))\n","\n","logits = network(ph_states)\n","\n","policy = tf.nn.softmax(logits)\n","log_policy = tf.nn.log_softmax(logits)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gkx-7eWVCCpk"},"source":["# Initialize model parameters\n","sess.run(tf.global_variables_initializer())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oi4aN8hgCCpm"},"source":["def predict_probs(states):\n","    \"\"\" \n","    Predict action probabilities given states.\n","    :param states: numpy array of shape [batch, state_shape]\n","    :returns: numpy array of shape [batch, n_actions]\n","    \"\"\"\n","    return policy.eval({ph_states: [states]})[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T0sePpnuCCpp"},"source":["### Play the game\n","\n","We can now use our newly built agent to play the game."]},{"cell_type":"code","metadata":{"id":"CFL2tBxtCCpq"},"source":["def generate_session(env, t_max=1000):\n","    \"\"\" \n","    Play a full session with REINFORCE agent.\n","    Returns sequences of states, actions, and rewards.\n","    \"\"\"\n","    # arrays to record session\n","    states, actions, rewards = [], [], []\n","    s = env.reset()\n","\n","    for t in range(t_max):\n","        # action probabilities array aka pi(a|s)\n","        action_probs = predict_probs(s)\n","\n","        # Sample action with given probabilities.\n","        a = np.random.choice(n_actions, 1, p=action_probs)[0]\n","        new_s, r, done, info = env.step(a)\n","\n","        # record session history to train later\n","        states.append(s)\n","        actions.append(a)\n","        rewards.append(r)\n","\n","        s = new_s\n","        if done:\n","            break\n","\n","    return states, actions, rewards"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ds3TQ1_xCCps"},"source":["# test it\n","states, actions, rewards = generate_session(env)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PMfhEevMCCpu"},"source":["### Computing cumulative rewards\n","\n","$$\n","\\begin{align*}\n","G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n","&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n","&= r_t + \\gamma * G_{t + 1}\n","\\end{align*}\n","$$"]},{"cell_type":"code","metadata":{"id":"2FNNtHhpCCpv"},"source":["from collections import deque\n","\n","def get_cumulative_rewards(rewards,  # rewards at each step\n","                           gamma=0.99  # discount for reward\n","                           ):\n","    \"\"\"\n","    Take a list of immediate rewards r(s,a) for the whole session \n","    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n","    \n","    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n","\n","    A simple way to compute cumulative rewards is to iterate from the last\n","    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n","\n","    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n","    \"\"\"\n","    cumulative_rewards = deque([rewards[-1]])\n","    for i in range(len(rewards)-2, -1, -1):\n","        cumulative_rewards.appendleft(rewards[i]+gamma*cumulative_rewards[0])\n","\n","    return cumulative_rewards"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CBLmB3TGCCpx","executionInfo":{"status":"ok","timestamp":1596470447283,"user_tz":420,"elapsed":466,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"a205b8c2-a02b-4880-ebd3-af9ebd3c4362","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["assert len(get_cumulative_rewards(range(100))) == 100\n","assert np.allclose(\n","    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n","    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n","assert np.allclose(\n","    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n","    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n","assert np.allclose(\n","    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n","    [0, 0, 1, 2, 3, 4, 0])\n","print(\"looks good!\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["looks good!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"L1RcwUc6CCp0"},"source":["#### Loss function and updates\n","\n","We now need to define objective and update over policy gradient.\n","\n","Our objective function is\n","\n","$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n","\n","REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n","\n","$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n","\n","We can abuse Tensorflow's capabilities for automatic differentiation by defining our objective function as follows:\n","\n","$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n","\n","When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient."]},{"cell_type":"code","metadata":{"id":"ftfn_DkWCCp0"},"source":["# This code selects the log-probabilities (log pi(a_i|s_i)) for those actions that were actually played.\n","indices = tf.stack([tf.range(tf.shape(log_policy)[0]), ph_actions], axis=-1)\n","log_policy_for_actions = tf.gather_nd(log_policy, indices)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qf1wMkiOCCp2"},"source":["# Policy objective as in the last formula. Please use reduce_mean, not reduce_sum.\n","# You may use log_policy_for_actions to get log probabilities for actions taken.\n","# Also recall that we defined ph_cumulative_rewards earlier.\n","\n","J = tf.reduce_mean(log_policy_for_actions*ph_cumulative_rewards)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P16zHEnWCCp5"},"source":["As a reminder, for a discrete probability distribution (like the one our policy outputs), entropy is defined as:\n","\n","$$ \\operatorname{entropy}(p) = -\\sum_{i = 1}^n p_i \\cdot \\log p_i $$"]},{"cell_type":"code","metadata":{"id":"Ma5LB0SKCCp5"},"source":["# Entropy regularization. If you don't add it, the policy will quickly deteriorate to\n","# being deterministic, harming exploration.\n","\n","entropy = -tf.reduce_sum(policy * log_policy, 1, name=\"entropy\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_YHmnrPqCCp8"},"source":["# # Maximizing X is the same as minimizing -X, hence the sign.\n","loss = -(J + 0.1 * entropy)\n","\n","update = tf.train.AdamOptimizer().minimize(loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m5hoMjfDCCp-"},"source":["def train_on_session(states, actions, rewards, t_max=1000):\n","    \"\"\"given full session, trains agent with policy gradient\"\"\"\n","    cumulative_rewards = get_cumulative_rewards(rewards)\n","    update.run({\n","        ph_states: states,\n","        ph_actions: actions,\n","        ph_cumulative_rewards: cumulative_rewards,\n","    })\n","    return sum(rewards)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6XNQrNxYCCqA"},"source":["# Initialize optimizer parameters\n","sess.run(tf.global_variables_initializer())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ni1ZBUoaCCqC"},"source":["### The actual training"]},{"cell_type":"code","metadata":{"id":"dZH2S8CmCCqD","executionInfo":{"status":"ok","timestamp":1596470666309,"user_tz":420,"elapsed":41378,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"12a02306-fd2e-4ffc-e7dc-3fb913f602a7","colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["for i in range(100):\n","    rewards = [train_on_session(*generate_session(env)) for _ in range(100)]  # generate new sessions\n","\n","    print(\"mean reward: %.3f\" % (np.mean(rewards)))\n","\n","    if np.mean(rewards) > 300:\n","        print(\"You Win!\")  # but you can train even further\n","        break"],"execution_count":null,"outputs":[{"output_type":"stream","text":["mean reward: 24.150\n","mean reward: 100.770\n","mean reward: 195.770\n","mean reward: 484.300\n","You Win!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MELC38ZMCCqF"},"source":["### Results & video"]},{"cell_type":"code","metadata":{"id":"IC2mxJuNCCqG"},"source":["# Record sessions\n","\n","import gym.wrappers\n","\n","with gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True) as env_monitor:\n","    sessions = [generate_session(env_monitor) for _ in range(100)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_iDB4AUXCCqI","executionInfo":{"status":"ok","timestamp":1596470707808,"user_tz":420,"elapsed":21542,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"84983e81-3253-4076-b90f-96aa57ff3cbc","colab":{"resources":{"http://localhost:8080/videos/openaigym.video.0.120.video000064.mp4":{"data":"CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K","ok":false,"headers":[["content-length","1449"],["content-type","text/html; charset=utf-8"]],"status":404,"status_text":""}},"base_uri":"https://localhost:8080/","height":501}},"source":["# Show video. This may not work in some setups. If it doesn't\n","# work for you, you can download the videos and view them locally.\n","\n","from pathlib import Path\n","from IPython.display import HTML\n","\n","video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n","\n","HTML(\"\"\"\n","<video width=\"640\" height=\"480\" controls>\n","  <source src=\"{}\" type=\"video/mp4\">\n","</video>\n","\"\"\".format(video_names[-1]))  # You can also try other indices"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","<video width=\"640\" height=\"480\" controls>\n","  <source src=\"videos/openaigym.video.0.120.video000064.mp4\" type=\"video/mp4\">\n","</video>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"-gr5SX-oCCqK"},"source":["from submit import submit_cartpole\n","submit_cartpole(generate_session, 'jeyasri.be@gmail.com', 'BNB72pimdAkKftud')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mKEOZfkWCCqM"},"source":["That's all, thank you for your attention!\n","\n","Not having enough? There's an actor-critic waiting for you in the honor section. But make sure you've seen the videos first."]}]}